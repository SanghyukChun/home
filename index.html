<!doctype html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-42711199-4"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-42711199-4');
        </script>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Bootstrap CSS -->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>

        <link href="./assets/css/common.css" rel="stylesheet">
        <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
        <link rel="icon" type="image/png" href="favicon.png">

        <!-- Roboto -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">

        <title>Sanghyuk Chun</title>
    </head>
    <body>
        <div class="container-wrapper">
            <div class="container">
                <div class="mt-5 mb-3">
                    <div class="row profile_header">
                        <div class="col-md-9 col-8">
                            <h1 class="d-none d-sm-block">Sanghyuk Chun <span><small><small>(He/Him)</small></small></span></h1>
                            <h3 class="d-block d-sm-none">Sanghyuk Chun</h3>
                            <ul class="list-unstyled">
                                <li>Lead Research Scientist</li>
                                <li>NAVER AI Lab</li>
                                <li>sanghyuk.chun [at] gmail.com</li>
                                <!--<li>Computer Science PhD Student (2025-Present)</li>-->
                                <!--<li>Princeton University</li>-->
                                <!--<li>sanghyukc [at] princeton.edu</li>-->
                                <li><a href="https://scholar.google.co.kr/citations?user=4_uj0xcAAAAJ">Google Scholar</a> | <a href="https://github.com/SanghyukChun">Github</a> | <a href="https://twitter.com/SanghyukChun">Twitter</a> | <a href="media/CV_Sanghyuk_Chun_Public_05Dec23.pdf">CV</a></li>
                            </ul>
                        </div>
                        <div class="col-md-3 col-4" style="display: flex; align-items: center;">
                            <img  src="assets/img/profile4.jpeg" alt="profile" class="profile rounded-circle shadow-lg">
                        </div>
                    </div>
                </div>

                <div class="mt-2 mb-5">
                    <p>I am currently serving as a lead research scientist at <a href="https://naver-career.gitbook.io/en/positions/ai-ml/ml-research">ML Research</a> team in <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab">NAVER AI Lab</a>, where my focus lies in the domains of machine learning, multi-modal learning (e.g., vision-language, language-audio, and audio-visual), and computer vision. At NAVER, my primary research goal aims to the development of generalizable machine learning models to challenging yet practical scenarios. Prior to joining NAVER, I held a position as a research engineer at KAKAO Corp from 2016 to 2018, where my work focused on recommendation systems and machine learning applications.</p>
                </div>
                <nav class="navbar navbar-expand sticky-top navbar-light bg-white top-bottom-border">
                    <ul class="navbar-nav me-auto mb-0">
                        <li class="nav-item">
                            <a class="nav-link active ps-0 pe-4" aria-current="page" href="#research">Research</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active ps-0 pe-4" aria-current="page" href="#papers">Publications</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active ps-0 pe-4" aria-current="page" href="#activities">Activities</a>
                        </li>
                        <!--<li class="nav-item">-->
                            <!--<a class="nav-link active ps-0 pe-4" aria-current="page" href="#random">Random</a>-->
                        <!--</li>-->
                    </ul>
                </nav>

                <h3 id="news" class="mt-5">News</h3>
                <div class="mb-5">
                    <ul class="pl15 mb-0">
                        <li>_4/2025 : Giving a talk at Yonsei University (topic: Towards Reliable and Efficient Multimodal AI) <a href="https://docs.google.com/presentation/d/1yQ7eC_GL9IOF1lUwYQPL7F8kx9ntM-bgl3dOeFJVLV4/edit?usp=sharing">[slide]</a></li>
                        <li>_3/2025 : 1 paper <sup><a href="#longprolip-a-probabilistic-vision-language-model-with-long-conte">[LongProLIP]</a></sup> is accepted at <a href="https://uncertainty-foundation-models.github.io/">ICLR 2025 Workshop on Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI</a>.</li>
                        <li>_2/2025 : Serving as an area chair at <a href="https://neurips.cc/Conferences/2025">NeurIPS 2025</a></li>
                        <li>_1/2025 : 1 paper <sup><a href="#probabilistic-language-image-pre-training">[ProLIP]</a></sup> is accepted at ICLR 2025.</li>
                        <li>_1/2025: <strong class="text-success">Reaching a research milestone of 10,000 citations</strong> at <a href="https://scholar.google.co.kr/citations?user=4_uj0xcAAAAJ">Google Scholar</a>!</li>
                    </ul>
                    <details>
                    <summary><strong>See older news</strong></summary>
                    <ul class="pl15 mb-0">
                        <li>12/2024 : Giving a talk at POSTECH AI Day (topic: Probabilistic Language-Image Pre-training) <a href="https://docs.google.com/presentation/d/1BEHEphXxdg0TjUsI3Cv8Xr3kLX6sbAlGytDEiN5iW7s/edit?usp=sharing">[slide]</a></li>
                        <li>12/2024 : Serving as an area chair at <a href="https://icml.cc/Conferences/2025">ICML 2025</a></li>
                        <li>12/2024 : 1 paper <sup><a href="#read-watch-and-scream-sound-generation-from-text-and-video">[ReWaS]</a></sup> is accepted at AAAI 2025.</li>
                        <li>11/2024 : 1 paper <sup><a href="#fairdro-group-fairness-regularization-via-classwise-robust-optim">[FairDRO extension]</a></sup> is accepted at <a href="https://www.sciencedirect.com/journal/neural-networks">Neural Networks</a>.</li>
                        <li>10/2024 : 1 paper <sup><a href="#read-watch-and-scream-sound-generation-from-text-and-video">[ReWaS]</a></sup> is accepted at NeurIPS 2024 Workshopon Video-Language Models.</li>
                        <li>10/2024 : Serving as an area chair at <a href="https://aistats.org/aistats2025/">AISTATS 2025</a></li>
                        <li>_9/2024 : 1 paper <sup><a href="#do-counterfactually-fair-image-classifiers-satisfy-group-fairnes">[CKD]</a></sup> is accepted at NeurIPS 2024 D&amp;B track.</li>
                        <li>_9/2024 : Giving a talk at SKKU (topic: "Realistic challenges and limitations of AI") <a href="https://docs.google.com/presentation/d/1s_7f3Uu6CtYrucFYQLhCJyV8l3Nhz7QZebyxpi5IwLs/edit?usp=sharing">[slide]</a></li>
                        <li>_8/2024 : RoCOCO<sup><a href="#rococo-robust-benchmark-of-ms-coco-to-stress-test-robustness-of">[RoCOCO]</a></sup> is accepted as <a href="https://syntheticdata4cv.wordpress.com/">ECCV 2024 Synthetic Data for Computer Vision Workshop</a> and <strong class="text-danger">selected as Oral presentation!</strong></li>
                        <li>_8/2024 : Giving a talk at <a href="https://soict.hust.edu.vn/summer-school">HUST AI Summer School on "Generative AI"</a> (topic: "CompoDiff") <a href="https://docs.google.com/presentation/d/1GEVu5aZUxeJg3B6AU4iORlfQ1qCA5tIQjePkk0q6c1c/edit?usp=sharing">[slide]</a></li>
                        <li>_8/2024 : Serving as an area chair at <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a></li>
                        <li>_8/2024 : HYPE<sup><a href="#hype-hyperbolic-entailment-filtering-for-underspecified-images-a">[HYPE]</a></sup> is selected as <strong class="text-danger">Oral presentation at this ECCV!</strong></li>
                        <li>_7/2024 : 1 paper <sup><a href="#compodiff-versatile-composed-image-retrieval-with-latent-diffusi">[CompoDiff]</a></sup> is accepted at <a href="https://openreview.net/forum?id=mKtlzW0bWc">TMLR</a>.</li>
                        <li>_7/2024 : 3 papers <sup><a href="#hype-hyperbolic-entailment-filtering-for-underspecified-images-a">[HYPE]</a></sup><sup><a href="#similarity-of-neural-architectures-using-adversarial-attack-tran">[SAT]</a></sup><sup><a href="#learning-with-unmasked-tokens-drives-stronger-vision-learners">[LUT]</a></sup> are accepted at ECCV 2024.</li>
                        <li>_4/2024 : 1 paper <sup><a href="#compodiff-versatile-composed-image-retrieval-with-latent-diffusi">[CompoDiff]</a></sup> is accepted at <a href="https://syndata4cv.github.io/">CVPR 2024 SynData4CV Workshop</a>.</li>
                        <li>_4/2024 : Serving as an area chair at <a href="https://nips.cc/Conferences/2024/CallForDatasetsBenchmarks">NeurIPS 2024 Datasets and Benchmarks Track</a>.</li>
                        <li>_3/2024 : Serving as an area chair at <a href="https://nips.cc/Conferences/2024">NeurIPS 2024</a>.</li>
                        <li>_3/2024 : 1 paper <sup><a href="#toward-interactive-regional-understanding-in-vision-large-langua">[RegionVLP]</a></sup> is accepted at NAACL 2024 main track.</li>
                        <li>_3/2024 : Giving a talk at UNIST (topic: "Probabilistic Image-Text Representations") <a href="https://docs.google.com/presentation/d/1IB-2A8w--jjQ9TAp1Xn8ANkfq_NyXAKaHtX3dh2e9_4/edit?usp=sharing">[slide]</a></li>
                        <li>_2/2024 : 1 paper <sup><a href="#language-only-efficient-training-of-zero-shot-composed-image-ret">[LinCIR]</a></sup> is accepted at CVPR 2024.</li>
                        <li>_2/2024 : Giving a talk at <a href="https://www.theieie.org/events/?part=03&c_id=872">IEIE AI Signal Processing Society Winter School</a> (topic: "Probabilistic Image-Text Representations") <a href="https://docs.google.com/presentation/d/1yelrDSN11rnChAk-gtU2YzSNX49XPHFIgROzj_lRA4Q/edit?usp=sharing">[slide]</a></li>
                        <li>_1/2024 : 2 papers <sup><a href="#improved-probabilistic-image-text-representations">[PCME++]</a></sup><sup><a href="#what-does-automatic-differentiation-compute-for-neural-networks">[AD Correctness]</a></sup> are accepted at ICLR 2024. One paper<sup><a href="#improved-probabilistic-image-text-representations">[PCME++]</a></sup> is my <strong class="text-success">sole authored paper</strong> 🤗, and one paper<sup><a href="#what-does-automatic-differentiation-compute-for-neural-networks">[AD Correctness]</a></sup> is selected as <strong class="text-danger">spotlight! (Top-5% paper)</strong></li>
                        <li>12/2023 : Giving a talk at Dankook University (topic: "Probabilistic Image-Text Representations") <a href="https://docs.google.com/presentation/d/1IB-2A8w--jjQ9TAp1Xn8ANkfq_NyXAKaHtX3dh2e9_4/edit?usp=sharing">[slide]</a></li>
                        <li>12/2013: We finally released <a href="https://huggingface.co/datasets/navervision/SynthTriplets18M">SynthTriplets18 dataset</a>!</li>
                        <li>11/2013: Being nominated as <strong class="text-danger">NeurIPS 2023 top reviewers (10%)</strong> <a href="https://neurips.cc/Conferences/2023/ProgramCommittee#top-reivewers">[link]</a>.</li>
                        <li>_9/2023 : Giving a talk at <a href="https://soict.hust.edu.vn/summer-school">HUST AI Summer School on "Modern Machine Learning: Foundations and Applications"</a> (topic: "Probabilistic Image-Text Representations") <a href="https://docs.google.com/presentation/d/1IB-2A8w--jjQ9TAp1Xn8ANkfq_NyXAKaHtX3dh2e9_4/edit?usp=sharing">[slide]</a></li>
                        <li>_9/2023 : 1 paper <sup><a href="#improved-probabilistic-image-text-representations">[PCME++ short]</a></sup> is accepted at the non-archival track of <a href="https://iccv-clvl.github.io/2023/">ICCV 2023 Workshop on Closing The Loop Between Vision And Language (CLVL)</a>.</li>
                        <li>_8/2023 : Giving a talk at Yonsei University (topic: "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion") <a href="https://docs.google.com/presentation/d/1VTJlrHqnLAcQP3aHydnlFXNeZpsPMGa9-L-Oaigi_6M/edit?usp=sharing">[slide]</a></li>
                        <li>_7/2023 : 1 paper <sup><a href="#seit-storage-efficient-vision-training-with-tokens-using-1-of-pi">[SeiT]</a></sup> is accepted at ICCV 2023.</li>
                        <li>_7/2023 : Serving as a <a href="https://jmlr.org/tmlr/editorial-board.html">TMLR Action Editor</a>.</li>
                        <li>_6/2023 : Being nominated as a <strong class="text-danger">TMLR Expert Reviewer</strong> <a href="https://openreview.net/group?id=TMLR/Expert_Reviewers">[link]</a>.</li>
                        <li>_6/2023 : Giving a talk at Sogang University (topic: "Probabilistic Image-Text Representations") <a href="https://docs.google.com/presentation/d/1hLCAGuY3HuJYzo20Puugw79aFSAIz8BMej-WJxCIsJk/edit?usp=sharing">[slide]</a></li>
                        <li>_5/2023 : Serving as an area chair at <a href="https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks">NeurIPS 2023 Datasets and Benchmarks Track</a>.</li>
                        <li>_4/2023 : We released "Graphit: A Unified Framework for Diverse Image Editing Tasks" <a href="https://github.com/navervision/Graphit">[GitHub]</a> <sup><a href="#graphit-a-unified-framework-for-diverse-image-editing-tasks">[Graphit]</a></sup>, The technical report will be released soon!</li>
                        <li>_4/2023 : 1 paper <sup><a href="#three-recipes-for-better-3d-pseudo-gts-of-3d-human-mesh-estimati">[3D-Pseudo-Gts]</a></sup> is accepted at CVPR 2023 Workshop on Computer Vision for Mixed Reality (CV4MR).</li>
                        <li>_1/2023 : 1 paper <sup><a href="#re-weighting-based-group-fairness-regularization-via-classwise-r">[FairDRO]</a></sup> is accepted at ICLR 2023.</li>
                        <li>_9/2022 : Giving a talk at Sogang University (topic: "ECCV Caption") <a href="https://docs.google.com/presentation/d/1OKaWPlNblepiXF57oWs2miGgYb5kuu1qxNqV_-hDddU/edit?usp=sharing">[slide]</a></li>
                        <li>_9/2022 : 1 paper <sup><a href="#a-unified-analysis-of-mixed-sample-data-augmentation-a-loss-func">[MSDA theorem]</a></sup> is accepted at NeurIPS 2022.</li>
                        <li>_8/2022 : <strong class="text-success">Starting a new chapter in life with <a href="https://8uos.github.io/">Song Park</a></strong> 🤵❤️👰.</li>
                        <li>_7/2022 : 1 paper <sup><a href="#few-shot-font-generation-with-weakly-supervised-localized-repres">[LF-Font journal]</a></sup> is accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI).</li>
                        <li>_7/2022 : 2 papers <sup><a href="#eccv-caption-correcting-false-negatives-by-collecting-machine-an">[ECCV Caption]</a></sup> <sup><a href="#domain-generalization-by-mutual-information-regularization-with">[MIRO]</a></sup> are accepted at ECCV 2022.</li>
                        <li>_7/2022 : Giving a talk at UNIST AIGS (topic: "Towards Reliable Machine Learning: Challenges, Examples, Solutions") <a href="https://docs.google.com/presentation/d/1SK2XwkQX5TPbkObDGnGgY4LWg6K-0dUdqyuosyVv2EI/edit?usp=sharing">[slide]</a></li>
                        <li>_6/2022 : Giving a tutorial on "Shortcut learning in Machine Learning: Challenges, Analysis, Solutions" at <a href="https://facctconference.org/2022/schedule.html">FAccT 2022</a>. [ <a href="https://sites.google.com/view/facct22-shortcut-learning/home">tutorial homepage</a> | <a href="https://docs.google.com/presentation/d/1UP-unGwtOhiO5rihMzNaXSCtn9fF7J5gLqmsY6jLve0/edit?usp=sharing">slide</a> | <a href="https://www.youtube.com/watch?v=8830gv2mIss">video</a> ]</li>
                        <li>_5/2022 : Receiving <strong><span class="text-danger">an outstanding reviewer award</span></strong> at CVPR 2022 <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">[link]</a>.</li>
                        <li>_5/2022 : 1 paper <sup><a href="#dataset-condensation-with-contrastive-signals">[DCC]</a></sup> is accepted at ICML 2022.</li>
                        <li>_4/2022 : 1 paper <sup><a href="#evaluation-for-weakly-supervised-object-localization-protocol-me">[WSOL Eval journal]</a></sup> is accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI).</li>
                        <li>_4/2022 : Organizing ICLR 2022 ML in Korea Social</li>
                        <li>_3/2022 : Giving guest lectures at KAIST and SNU (topic: "Towards Reliable Machine Learning") <a href="https://docs.google.com/presentation/d/1Z1XNVl6LfslCyTsERQQRzTb6qMpuyc-8wkMIYuu7IAY/edit?usp=sharing">[slide]</a></li>
                        <li>_3/2022 : Co-organizing FAccT 2022 Translation/Dialogue Tutorial: "Shortcut learning in Machine Learning: Challenges, Analysis, Solutions" (slides, videos and web pages will be released soon)</li>
                        <li>_3/2022 : 1 paper <sup><a href="#learning-fair-classifiers-with-partially-annotated-group-labels">[CGL]</a></sup> is accepted at CVPR 2022.</li>
                        <li>_2/2022 : Giving a talk at <a href="https://sites.google.com/view/pair-ml-winter-seminar-2022/home">POSTECH AI Research (PAIR) ML Winter Seminar 2022</a> (topic: "Shortcut learning in Machine Learning: Challenges, Examples, Solutions") <a href="https://docs.google.com/presentation/d/1LGtjxaXwkk_Z6OaJFUEjputi6eEtkq3h1sXB2yx7gPk/edit?usp=sharing">[slide]</a></li>
                        <li>_1/2022 : 2 papers <sup><a href="#vidt-an-efficient-and-effective-fully-transformer-based-object-d">[ViDT]</a></sup> <sup><a href="#which-shortcut-cues-will-dnns-choose-a-study-from-the-parameter">[WCST-ML]</a></sup> are accepted at ICLR 2022.</li>
                        <li>12/2021 : Co-hosting NeurIPS'21 workshop on <a href="#neurips-2021-workshop-on-imagenet-past-present-and-future">ImageNet: Past, Present, and Future</a> with 400+ attendees!</li>
                        <li>12/2021 : Giving a talk at University of Seoul (topic: "Realistic challenges and limitations of AI") <a href="https://docs.google.com/presentation/d/1FOIMm4bYWN6b80_H0El1N4PzGu-2gfNqfR6bgjU8-58/edit?usp=sharing">[slide]</a></li>
                        <li>11/2021 : Giving a talk at NAVER and NAVER Labs Europe (topic: Mitigating dataset biases in Real-world ML applications) <a href="https://docs.google.com/presentation/d/1jyrfZfSEwbgzSuVKUURfmIEo3baksOL2GxSrBSzR4qo/edit">[slide]</a></li>
                        <li>11/2021 : Giving a guest lecture at UNIST (topic: Limits and Challenges in Deep Learning Optimizers) <a href="https://docs.google.com/presentation/d/1NNftqS6BcCPd52tv8gWEjB34retYhP0FToOFBd9ewkQ/edit">[slide]</a></li>
                        <li>10/2021 : Releasing an unified few-shot font generation framework! <a href="https://github.com/clovaai/fewshot-font-generation">[code]</a></li>
                        <li>_9/2021 : 2 papers <sup><a href="#swad-domain-generalization-by-seeking-flat-minima">[SWAD]</a></sup> <sup><a href="#neural-hybrid-automata-learning-dynamics-with-multiple-modes-and">[NHA]</a></sup> are accepted at NeurIPS 2021.</li>
                        <li>_8/2021: <strong class="text-success">Reaching a research milestone of 1,000 citations</strong> at <a href="https://scholar.google.co.kr/citations?user=4_uj0xcAAAAJ">Google Scholar</a> and <a href="https://www.semanticscholar.org/author/Sanghyuk-Chun/2647582">Semantic Scholar</a>!</li>
                        <li>_7/2021 : Co-organizing the NeurIPS Workshop on ImageNet: Past, Present, and Future! <a href="https://sites.google.com/view/imagenet-workshop/home">[webpage]</a></li>
                        <li>_7/2021 : 2 papers <sup><a href="#multiple-heads-are-better-than-one-few-shot-font-generation-with">[MX-Font]</a></sup> <sup><a href="#rethinking-spatial-dimensions-of-vision-transformers">[PiT]</a></sup> are accepted at ICCV 2021.</li>
                        <li>_7/2021 : Giving a talk at Computer Vision Centre (CVC), UAB (topic: PCME and AdamP) <a href="http://www.cvc.uab.es/?p=7778">[info]</a> <a href="https://docs.google.com/presentation/d/1dGQUqud3iMld-UgMMlRQuqA7JhzndMfzFoKaFAwZ58I/edit?usp=sharing">[slide]</a></li>
                        <li>_6/2021 : Giving a talk at KSIAM 2021 (topic: AdamP). <a href="https://docs.google.com/presentation/d/1s9zgQ22WFnhEj6POL_0ecrTiED__qmL9XgL0Nv3zNP4/edit?usp=sharing">[slide]</a></li>
                        <li>_6/2021 : Giving a guest lecture at Seoul National University (topic: few-shot font generation) .<a href="https://docs.google.com/presentation/d/13WoYS9r9C751s3nZ2yF5WdestRXh3V7LFQDOJyCrt9s/edit?usp=sharing">[slide]</a></li>
                        <li>_5/2021 : Receiving <strong><span class="text-danger">an outstanding reviewer award</span></strong> at CVPR 2021 <a href="https://cvpr2021.thecvf.com/node/184">[link]</a>.</li>
                        <li>_4/2021 : 1 paper <sup><a href="#few-shot-font-generation-with-localized-style-representations-an">[LF-Font]</a></sup> is accepted at CVPR 2021 workshop (also appeared at AAAI).</li>
                        <li>_3/2021 : 2 papers <sup><a href="#probabilistic-embeddings-for-cross-modal-retrieval">[PCME]</a></sup> <sup><a href="#re-labeling-imagenet-from-single-to-multi-labels-from-global-to">[ReLabel]</a></sup> are accepted at CVPR 2021.</li>
                        <li>_1/2021 : 1 paper <sup><a href="#adamp-slowing-down-the-slowdown-for-momentum-optimizers-on-scale">[AdamP]</a></sup> is accepted at ICLR 2021.</li>
                        <li>12/2020 : 1 paper <sup><a href="#few-shot-font-generation-with-localized-style-representations-an">[LF-Font]</a></sup> is accepted at AAAI 2021.</li>
                        <li>_7/2020 : 1 paper <sup><a href="#few-shot-compositional-font-generation-with-dual-memory">[DM-Font]</a></sup> is accepted at ECCV 2020.</li>
                        <li>_6/2020 : Receiving <strong><span class="text-danger">the best paper runner-up award</span></strong> at AICCW CVPR 2020 <sup><a href="#toward-high-quality-few-shot-font-generation-with-dual-memory">[DM-Font WS]</a></sup>.</li>
                        <li>_6/2020 : Receiving <strong><span class="text-danger">an outstanding reviewer award</span></strong> at CVPR 2020 <a href="https://cvpr2020.thecvf.com/reviewer-acknowledgements">[link]</a>.</li>
                        <li>_6/2020 : Giving a talk at CVPR 2020 NAVER interative session.</li>
                        <li>_6/2020 : 1 paper <sup><a href="#learning-de-biased-representations-with-biased-representations">[ReBias]</a></sup> is accepted at ICML 2020.</li>
                        <li>_4/2020 : 1 paper <sup><a href="#toward-high-quality-few-shot-font-generation-with-dual-memory">[DM-Font short]</a></sup> is accepted at CVPR 2020 workshop.</li>
                        <li>_2/2020 : 1 paper <sup><a href="#evaluating-weakly-supervised-object-localization-methods-right">[wsoleval]</a></sup> is accepted at CVPR 2020.</li>
                        <li>_1/2020 : 1 paper <sup><a href="#data-driven-harmonic-filters-for-audio-representation-learning">[HCNN]</a></sup> is accepted at ICASSP 2020.</li>
                        <li>10/2019 : 1 paper <sup><a href="#automatic-music-tagging-with-harmonic-cnn">[HCNN short]</a></sup> is accpeted at ISMIR late break demo.</li>
                        <li>10/2019 : Working at Naver Labs Europe as a visiting researcher (Oct - Dec 2019)</li>
                        <li>_7/2019 : 2 papers <sup><a href="#cutmix-regularization-strategy-to-train-strong-classifiers-with">[CutMix]</a> <a href="#photorealistic-style-transfer-via-wavelet-transforms">[WCT2]</a></sup> are accepted at ICCV 2019 (1 oral presentation).</li>
                        <li>_6/2019 : Giving a talk at ICML 2019 Expo workshop.</li>
                        <li>_5/2019 : 2 papers <sup><a href="#visualizing-and-understanding-self-attention-based-music-tagging">[MTSA]</a> <a href="#an-empirical-evaluation-on-robustness-and-uncertainty-of-regular">[RegEval]</a></sup> are accepted at ICML 2019 workshops (1 oral presentation).</li>
                        <li>_5/2019 : Giving a talk at ICLR 2019 Expo talk.</li>
                        <li>_3/2019 : 1 paper <sup><a href="#where-to-be-adversarial-perturbations-added-investigating-and-ma">[PRM]</a></sup> is accepted at ICLR 2019 workshop.</li>
                    </ul>
                    </details>
                </div>

                <div id="current_service" class="mt-5 mb-5">
                    <h3>Current Service Appointments</h3>
                    <ul>
                        <li><strong>Area Chair</strong> - <span class="badge bg-danger">NeurIPS 2025</span> <span class="badge bg-danger">ICML 2025</span> <span class="badge bg-danger">ICLR 2025</span> <span class="badge bg-danger">AISTATS 2025</span></li>
                        <li><strong>Action Editor</strong> - <span class="badge bg-success">TMLR</span></li>
                        <li><strong>Reviewer</strong> - <span class="badge bg-warning">ICLR Blog track 2025</span> <span class="badge bg-warning">CVPR 2025</span> <span class="badge bg-warning">ICCV 2025</span> </li>
                    </ul>
                </div>

                <hr>
                <h3 id="papers" class="mt-5">Research</h3>
                <p class="text-justified">I am interested in machine learning and its reliability. My research philosophy aims to (1) scalability and (2) theoretical or conceptual soundness. Recently, I have focused on multi-modal learning (e.g., vision-language and audio-visual), but I am open to exploring a broader range of topics. More details about my research can be found in my <a href="media/research_statement.pdf">research statement</a> (as of December 2023). Representative works that reflect my research approach and significant contributions are <span class="li-selected">highlighted</span>, with the most significant works emphasized in <span class="li-selected-two">orange</span>.</p>
                <p>
                    <strong>(C: peer-reviewed conference, W: peer-reviewed workshop, A: arxiv preprint, O: others)<br>
                   <span class="text-danger">(<sup>&#10059;</sup>authors contributed equally)</strong></span><br>
                    See also at my <a href="https://scholar.google.co.kr/citations?user=4_uj0xcAAAAJ">Google Scholar</a>.
                </p>

                <div class="card no-border mb-4">
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2024">2025</h4>
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">

                        <li class="list-group-item li-workshop li-selected">
                            <strong class="anchor-strong">LongProLIP: A Probabilistic Vision-Language Model with Long Context Text.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Sangdoo Yun</li>
                                <li><strong><em>ICLR 2025 QUESTION Workshop.</em></strong> <a href="media/papers/chun2025iclrws_longprolip.pdf">paper</a> | <a href="https://github.com/naver-ai/prolip/">code</a> | <a href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">pre-trained models 🤗</a> | <a href="https://docs.google.com/presentation/d/1BEHEphXxdg0TjUsI3Cv8Xr3kLX6sbAlGytDEiN5iW7s/edit?usp=sharing">slide</a> | <a href="media/bibtex/chun2025iclrws_longprolip.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-arxiv li-selected">
                            <strong class="anchor-strong">DNNs May Determine Major Properties of Their Outputs Early, with Timing Possibly Driven by Bias.</strong>
                            <ul>
                                <li>Song Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun<sup>&#10059;</sup></strong>, Byeongho Heo, Dongyoon Han</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/park2025early_determination.pdf">paper</a> | <a href="media/bibtex/park2025early_determination.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">Probabilistic Language-Image Pre-Training.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Wonjae Kim, Song Park, Sangdoo Yun</li>
                                <li><strong><em>ICLR 2025.</em></strong> <a href="media/papers/chun2025iclr_prolip.pdf">paper</a> | <a href="https://github.com/naver-ai/prolip/">code</a> | <a href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">pre-trained models 🤗</a> | <a href="https://docs.google.com/presentation/d/1BEHEphXxdg0TjUsI3Cv8Xr3kLX6sbAlGytDEiN5iW7s/edit?usp=sharing">slide</a> | <a href="media/bibtex/chun2025iclr_prolip.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference-workshop">
                            <strong class="anchor-strong">Read, Watch and Scream! Sound Generation from Text and Video.</strong>
                            <ul>
                                <li>Yujin Jeong, Yunji Kim, <strong>Sanghyuk Chun</strong>, Jiyoung Lee</li>
                                <li><strong><em>AAAI 2025 | NeurIPS 2024 Workshop on Video-Language Models.</em></strong> <a href="media/papers/jeong2025aaai_rewas.pdf">paper</a> | <a href="https://github.com/naver-ai/rewas">code</a> | <a href="https://naver-ai.github.io/rewas/">project page</a> | <a href="media/bibtex/jeong2025aaai_rewas.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-journal">
                            <strong class="anchor-strong">FairDRO: Group Fairness Regularization via Classwise Robust Optimization.</strong>
                            <ul>
                                <li>Taeeon Park, Sangwon Jung, <strong>Sanghyuk Chun</strong>, Taesup Moon</li>
                                <li><strong><em>Neural Networks.</em></strong> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024008207?CMX_ID=&SIS_ID=&dgcid=STMJ_219742_AUTH_SERV_PA&utm_acid=188087385&utm_campaign=STMJ_219742_AUTH_SERV_PA&utm_in=DM523185&utm_medium=email&utm_source=AC_">paper</a> | <a href="https://github.com/sangwon-jung94/FairDRO">code</a> | <a href="media/bibtex/park2024fairdro_journal.txt">bibtex</a></li>
                            </ul>
                        </li>

                    </ul>
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2024">2024</h4>
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Do Counterfactually Fair Image Classifiers Satisfy Group Fairness? -- A Theoretical and Empirical Study.</strong>
                            <ul>
                                <li>Sangwon Jung<sup>&#10059;</sup>, Sumin Yu<sup>&#10059;</sup>, <strong>Sanghyuk Chun<sup>&dagger;</sup></strong>, Taesup Moon<sup>&dagger;</sup></li>
                                <li><strong><em>NeurIPS 2024 D&B.</em></strong> <a href="media/papers/jung2024neurips_ckd.pdf">paper</a> | <a href="https://github.com/sumin-yu/CKD">code and dataset</a> | <a href="media/bibtex/jung2024neurips_ckd.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts.</strong> <span class="badge bg-danger">Oral presentation</span>
                            <ul>
                                <li>Wonjae Kim, <strong>Sanghyuk Chun</strong>, Taekyung Kim, Dongyoon Han, Sangdoo Yun</li>
                                <li><strong><em>ECCV 2024.</em></strong> <a href="media/papers/kim2024hype.pdf">paper</a> | <a href="https://github.com/naver-ai/hype">code</a> | <a href="media/bibtex/kim2024hype.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Similarity of Neural Architectures using Adversarial Attack Transferability.</strong>
                            <ul>
                                <li>Jaehui Hwang, Dongyoon Han, Byeongho Heo, Song Park, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Jong-Seok Lee<sup>&#10059;</sup></li>
                                <li><strong><em>ECCV 2024.</em></strong> <a href="media/papers/hwang2024sat.pdf">paper</a> | <a href="https://github.com/J-H-Hwang/SAT">code</a> | <a href="media/bibtex/hwang2024sat.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Learning with Unmasked Tokens Drives Stronger Vision Learners.</strong>
                            <ul>
                                <li>Taekyung Kim<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong>, Byeongho Heo, Dongyoon Han<sup>&#10059;</sup></li>
                                <li><strong><em>ECCV 2024.</em></strong> <a href="media/papers/kim2024lut.pdf">paper</a> | <a href="https://github.com/naver-ai/lut">code</a> | <a href="media/bibtex/kim2024lut.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">RoCOCO: Robust Benchmark of MS-COCO to Stress-test Robustness of Image-Text Matching Models.</strong> <span class="badge bg-danger">Oral presentation</span>
                            <ul>
                                <li>Seulki Park, Daeho Um, Hajung Yoon, <strong>Sanghyuk Chun</strong>, Sangdoo Yun</li>
                                <li><strong><em>ECCV 2024 Synthetic Data for Computer Vision Workshop (SyntheticData4CV 2024).</em></strong> <a href="media/papers/park2023rococo.pdf">paper</a> | <a href="https://github.com/pseulki/rococo">code</a> | <a href="media/bibtex/park2023rococo.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-journal li-selected">
                            <strong class="anchor-strong">CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion.</strong>
                            <ul>
                                <li>Geonmo Gu<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun</li>
                                <li><strong><em>TMLR.</em></strong> <strong><em>CVPR 2024 SynData4CV Workshop.</em></strong> <a href="media/papers/gu2024compodiff.pdf">paper</a> | <a href="https://github.com/navervision/CompoDiff">code</a> | <a href="https://huggingface.co/datasets/navervision/SynthTriplets18M">SynthTriplets18M dataset</a> | <a href="https://huggingface.co/spaces/navervision/CompoDiff-Aesthetic">demo 🤗</a> | <a href="https://docs.google.com/presentation/d/1VTJlrHqnLAcQP3aHydnlFXNeZpsPMGa9-L-Oaigi_6M/edit?usp=sharing">slide</a> | <a href="media/bibtex/gu2024compodiff.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders for Composed Image Retrieval.</strong>
                            <ul>
                                <li>Jaeseok Byun<sup>&#10059;</sup>, Seokhyeon Jeong<sup>&#10059;</sup>, Wonjae Kim, <strong>Sanghyuk Chun</strong><sup>&dagger;</sup>, Taesup Moon<sup>&dagger;</sup></li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/byun2024rtd.pdf">paper</a> | <a href="media/bibtex/byun2024rtd.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Toward Interactive Regional Understanding in Vision-Large Language Models</strong>
                            <ul>
                                <li>Jungbeom Lee, <strong>Sanghyuk Chun<sup>&#10059;</sup></strong>, Sangdoo Yun<sup>&#10059;</sup></li>
                                <li><strong><em>NAACL 2024.</em></strong> <a href="media/papers/lee2024naacl_region_vlm.pdf">paper</a> | <a href="https://github.com/jbeomlee93/RegionVLM">code</a> | <a href="media/bibtex/lee2024vlm.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Language-only Efficient Training of Zero-shot Composed Image Retrieval.</strong>
                            <ul>
                                <li>Geonmo Gu<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Wonjae Kim, Yoohoon Kang, Sangdoo Yun</li>
                                <li><strong><em>CVPR 2024.</em></strong> <a href="media/papers/gu2024cvpr_lincir.pdf">paper</a> | <a href="https://github.com/navervision/lincir">code</a> | <a href="media/bibtex/gu2024lincir.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">What Does Automatic Differentiation Compute for Neural Networks?</strong> <span class="badge bg-info">Spotlight presentation</span>
                            <ul>
                                <li>Sejun Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun<sup>&#10059;</sup></strong>, Wonyeol Lee</li>
                                <li><strong><em>ICLR 2024.</em></strong> <a href="media/papers/park2024ad_correctness.pdf">paper</a> | <a href="https://github.com/SanghyukChun/ad_correctness">code</a> | <a href="media/bibtex/park2024ad_correctness.txt">bibtex</a></li>
                            </ul>
                        </li>
                        <li class="list-group-item li-conference-workshop li-selected-two">
                            <strong class="anchor-strong">Improved Probabilistic Image-Text Representations.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong></li>
                                <li><strong><em>ICLR 2024.</em></strong> <strong><em>ICCV CLVL 2023.</em></strong> <a href="media/papers/chun2024iclr_pcmepp.pdf">paper</a> | <a href="https://github.com/naver-ai/pcmepp/">code</a> | <a href="https://naver-ai.github.io/pcmepp/">project page</a> | <a href="https://docs.google.com/presentation/d/1yelrDSN11rnChAk-gtU2YzSNX49XPHFIgROzj_lRA4Q/edit?usp=sharing">slide</a> | <a href="media/bibtex/chun2024iclr_pcmepp.txt">bibtex</a></li>
                            </ul>
                        </li>

                    </ul>

                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2023">2023</h4>
                        <!--<span class="float-left"><strong> &nbsp; &nbsp; 1 conference papers (1 ICLR)</strong></span>-->
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage.</strong>
                            <ul>
                                <li>Song Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Byeongho Heo, Wonjae Kim, Sangdoo Yun</li>
                                <li><strong><em>ICCV 2023.</em></strong> <a href="media/papers/park2023seit.pdf">paper</a> | <a href="https://github.com/naver-ai/seit">code</a> | <a href="https://docs.google.com/presentation/d/1oHjYjvbC3QINuwR5MQAteik0ifsIxR3LTuCus-BTKwY/edit#slide=id.p">slide</a> | <a href="media/bibtex/park2023seit.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">Three Recipes for Better 3D Pseudo-GTs of 3D Human Mesh Estimation in the Wild.</strong>
                            <ul>
                                <li>Gyeongsik Moon, Hongsuk Choi, <strong>Sanghyuk Chun</strong>, Jiyoung Lee, Sangdoo Yun</li>
                                <li><strong><em>CVPR 2023 Workshop on Computer Vision for Mixed Reality (CV4MR).</em></strong> <a href="media/papers/moon2023pseudo_gt_recipes.pdf">paper</a> | <a href="https://github.com/mks0601/NeuralAnnot_RELEASE">code</a> | <a href="media/bibtex/moon2023pseudo_gt_recipes.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Re-weighting based Group Fairness Regularization via Classwise Robust Optimization.</strong>
                            <ul>
                                <li>Sangwon Jung<sup>&#10059;</sup>, Taeeon Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong>, Taesup Moon</li>
                                <li><strong><em>ICLR 2023.</em></strong> <a href="media/papers/jung2023iclr_fairdro.pdf">paper</a> | <a href="https://github.com/sangwon-jung94/FairDRO">code</a> | <a href="media/bibtex/jung2023iclr_fairdro.txt">bibtex</a></li>
                            </ul>
                        </li>

                    </ul>
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2022">2022</h4>
                        <!--<span class="float-left"><strong> &nbsp; &nbsp; 7 conference papers (2 ICLR, 1 CVPR, 1 ICML, 2 ECCV, 1 NeurIPS), 2 journal papers (WSOL eval journal, LF-Font journal)</strong></span>-->
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">Group Generalized Mean Pooling for Vision Transformer.</strong>
                            <ul>
                                <li>Byungsoo Ko, Han-Gyu Kim, Byeongho Heo, Sangdoo Yun, <strong>Sanghyuk Chun</strong>, Geonmo Gu, Wonjae Kim</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/ko2022ggem.pdf">paper</a> | <a href="media/bibtex/ko2022ggem.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective.</strong>
                            <ul>
                                <li>Chanwoo Park<sup>&#10059;</sup>, Sangdoo Yun<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong></li>
                                <li><strong><em>NeurIPS 2022.</em></strong> <a href="media/papers/park2022msda.pdf">paper</a> | <a href="https://github.com/naver-ai/hmix-gmix">code</a> | <a href="media/bibtex/park2022neurips_msda.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Wonjae Kim, Song Park, Minsuk Chang, Seong Joon Oh</li>
                                <li><strong><em>ECCV 2022.</em></strong> <a href="media/papers/chun2022eccv_caption.pdf">paper</a> | <a href="https://github.com/naver-ai/eccv-caption">code</a> | <a href="https://pypi.org/project/eccv-caption/">pypi</a> | <a href="https://docs.google.com/presentation/d/1zyLL49_2-F6mQFaMIumPfdE7el_r048XtidLnehepHo/edit?usp=sharing">slide (short talk)</a> | <a href="https://docs.google.com/presentation/d/1OKaWPlNblepiXF57oWs2miGgYb5kuu1qxNqV_-hDddU/edit?usp=sharing">slide (long talk)</a> | <a href="media/bibtex/chun2022eccv_caption.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Domain Generalization by Mutual-Information Regularization with Pre-trained Models.</strong>
                            <ul>
                                <li>Junbum Cha, Kyungjae Lee, Sungrae Park, <strong>Sanghyuk Chun</strong></li>
                                <li><strong><em>ECCV 2022.</em></strong> <a href="media/papers/cha2022miro.pdf">paper</a> | <a href="https://github.com/kakaobrain/miro">code</a> | <a href="media/bibtex/cha2022miro.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-journal">
                            <strong class="anchor-strong">Few-shot Font Generation with Weakly Supervised Localized Representations.</strong>
                            <ul>
                                <li>Song Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Junbum Cha, Bado Lee, Hyunjung Shim</li>
                                <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 2022. <strong><span class="text-danger">(IF:24.314)</span></strong></li>
                                <li><strong><em>PAMI.</em></strong> <a href="media/papers/park2021lffont_extension.pdf">paper</a> | <a href="https://github.com/clovaai/lffont">code (old)</a> | <a href="https://github.com/clovaai/fewshot-font-generation">code (new)</a> | <a href="https://cvml.yonsei.ac.kr/projects/few-shot-font-generation">project page</a> | <a href="media/bibtex/park2021lffont_extension.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-journal">
                            <strong class="anchor-strong">Evaluation for Weakly Supervised Object Localization: Protocol, Metrics, and Datasets.</strong>
                            <ul>
                                <li>Junsuk Choe<sup>&#10059;</sup>, Seong Joon Oh<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong>, Seungho Lee, Zeynep Akata, Hyunjung Shim</li>
                                <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 2022. <strong><span class="text-danger">(IF:24.314)</span></strong></li>
                                <li><strong><em>PAMI.</em></strong> <a href="media/papers/choe2020wsoleval_extension.pdf">paper</a> | <a href="https://github.com/ClovaAI/wsolevaluation">code and dataset</a> | <a href="media/bibtex/choe2020wsoleval_extension.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">An Extendable, Efficient and Effective Transformer-based Object Detector.</strong>
                            <ul>
                                <li>Hwanjun Song, Deqing Sun, <strong>Sanghyuk Chun</strong>, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/song2022vidtplus.pdf">paper</a> | <a href="https://github.com/naver-ai/vidt">code</a> | <a href="media/bibtex/song2022vidtplus.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Dataset Condensation with Contrastive Signals.</strong>
                            <ul>
                                <li>Saehyung Lee, <strong>Sanghyuk Chun</strong>, Sangwon Jung, Sangdoo Yun, Sungroh Yoon</li>
                                <li><strong><em>ICML 2022.</em></strong> <a href="media/papers/lee2022icml_dcc.pdf">paper</a> | <a href="media/bibtex/lee2022dcc.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Learning Fair Classifiers with Partially Annotated Group Labels.</strong>
                            <ul>
                                <li>Sangwon Jung, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Taesup Moon<sup>&#10059;</sup></li>
                                <li><strong><em>CVPR 2022.</em></strong> <a href="media/papers/jung2022cvpr_cgl.pdf">paper</a> | <a href="https://github.com/naver-ai/cgl_fairness">code</a> | <a href="media/bibtex/jung2022cgl.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">ViDT: An Efficient and Effective Fully Transformer-based Object Detector.</strong>
                            <ul>
                                <li>Hwanjun Song, Deqing Sun, <strong>Sanghyuk Chun</strong>, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang</li>
                                <li><strong><em>ICLR 2022.</em></strong> <a href="media/papers/song2022iclr_vidt.pdf">paper</a> | <a href="https://github.com/naver-ai/vidt">code</a> | <a href="media/bibtex/song2022vidt.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective.</strong>
                            <ul>
                                <li>Luca Scimeca<sup>&#10059;</sup>, Seong Joon Oh<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong>, Michael Poli, Sangdoo Yun</li>
                                <li><strong><em>ICLR 2022.</em></strong> <a href="media/papers/scimeca2021wcst-ml.pdf">paper</a> | <a href="media/bibtex/scimeca2022wcst-ml.txt">bibtex</a></li>
                            </ul>
                        </li>
                    </ul>
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2021">2021</h4>
                        <!--<span class="float-left"><strong> &nbsp; &nbsp; 8 conference papers and 1 workshop paper (1 AAAI, 1 ICLR, 2 CVPR, 1 CVPR Workshop, 2 ICCV, 2 NeurIPS)</strong></span>-->
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">SWAD: Domain Generalization by Seeking Flat Minima.</strong>
                            <ul>
                                <li>Junbum Cha, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Kyungjae Lee<sup>&#10059;</sup>, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, Sungrae Park</li>
                                <li><strong><em>NeurIPS 2021.</em></strong> <a href="media/papers/cha2021neurips_swad.pdf">paper</a> | <a href="https://github.com/khanrc/swad">code</a> | <a href="media/bibtex/cha2021neurips_swad.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions.</strong>
                            <ul>
                                <li>Michael Poli<sup>&#10059;</sup>, Stefano Massaroli<sup>&#10059;</sup>, Luca Scimeca, Seong Joon Oh, <strong>Sanghyuk Chun</strong>, Atsushi Yamashita, Hajime Asama, Jinkyoo Park, Animesh Garg</li>
                                <li><strong><em>NeurIPS 2021.</em></strong> <a href="media/papers/poli2021nha.pdf">paper</a> | <a href="media/bibtex/poli2021nha.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">StyleAugment: Learning Texture De-biased Representations by Style Augmentation without Pre-defined Textures.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Song Park</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/chun2021styleaugment.pdf">paper</a> | <a href="media/bibtex/chun2021styleaugment.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Rethinking Spatial Dimensions of Vision Transformers.</strong>
                            <ul>
                                <li>Byeongho Heo, Sangdoo Yun, Dongyoon Han, <strong>Sanghyuk Chun</strong>, Junsuk Choe, Seong Joon Oh</li>
                                <li><strong><em>ICCV 2021.</em></strong> <a href="media/papers/heo2021iccv_pit.pdf">paper</a> | <a href="https://github.com/naver-ai/pit">code</a> | <a href="https://twitter.com/SanghyukChun/status/1377125468999049216">tweet</a> | <a href="media/bibtex/heo2021pit.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts.</strong>
                            <ul>
                                <li>Song Park, <strong>Sanghyuk Chun</strong>, Junbum Cha, Bado Lee, Hyunjung Shim</li>
                                <li><strong><em>ICCV 2021.</em></strong> <a href="media/papers/park2021iccv_mxfont.pdf">paper</a> | <a href="https://github.com/clovaai/mxfont">code</a> | <a href="media/bibtex/park2021mxfont.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">Probabilistic Embeddings for Cross-Modal Retrieval.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Seong Joon Oh, Rafael Sampaio de Rezende, Yannis Kalantidis, Diane Larlus</li>
                                <li><strong><em>CVPR 2021.</em></strong> <a href="media/papers/chun2021cvpr_pcme.pdf">paper</a> | <a href="https://github.com/naver-ai/pcme">code</a> | <a href="https://www.youtube.com/watch?v=J_DaqSLEcVk">video</a> | <a href="https://docs.google.com/presentation/d/1Tyac3fRvEGYkmbB9iUELU5jjv8IWo5oU_eQUGOkiCuk/edit?usp=sharing">slide (short talk)</a> | <a href="https://docs.google.com/presentation/d/1dGQUqud3iMld-UgMMlRQuqA7JhzndMfzFoKaFAwZ58I/edit?usp=sharing">slide (long talk)</a> | <a href="media/bibtex/chun2021pcme.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels.</strong>
                            <ul>
                                <li>Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, <strong>Sanghyuk Chun</strong></li>
                                <li><strong><em>CVPR 2021.</em></strong> <a href="media/papers/yun2021cvpr_relabel.pdf">paper</a> | <a href="https://github.com/naver-ai/relabel_imagenet">code</a> | <a href="media/bibtex/yun2021relabel.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights.</strong>
                            <ul>
                                <li>Byeongho Heo<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha</li>
                                <li><strong><em>ICLR 2021.</em></strong> <a href="media/papers/heo2021iclr_adamp.pdf">paper</a> | <a href="https://github.com/ClovaAI/AdamP">code</a> | <a href="https://clovaai.github.io/AdamP/">project page</a> | <a href="https://pypi.org/project/adamp/">pypi</a> | <a href="https://docs.google.com/presentation/d/1s9zgQ22WFnhEj6POL_0ecrTiED__qmL9XgL0Nv3zNP4/edit?usp=sharing">slide</a> | <a href="media/bibtex/heo2021adamp.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference-workshop li-selected">
                            <strong class="anchor-strong">Few-shot Font Generation with Localized Style Representations and Factorization.</strong>
                            <ul>
                                <li>Song Park<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Junbum Cha, Bado Lee, Hyunjung Shim</li>
                                <li><strong><em>AAAI 2021.</em></strong> <strong><em>CVPR Workshop 2021.</em></strong> <a href="media/papers/park2021aaai_lffont.pdf">paper</a> | <a href="https://github.com/clovaai/lffont">code</a> | <a href="https://cvml.yonsei.ac.kr/projects/few-shot-font-generation">project page</a> | <a href="media/bibtex/park2021lffont.txt">bibtex</a></li>
                            </ul>
                        </li>
                    </ul>
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2020">2020</h4>
                        <!--<span class="float-left"><strong> &nbsp; &nbsp; 4 conference papers, 1 workshop paper (1 ICASSP, 1 CVPR, 1 ICML, 1 CVPR WS, 1 ECCV)</strong></span>-->
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Few-shot Compositional Font Generation with Dual Memory.</strong>
                            <ul>
                                <li>Junbum Cha, <strong>Sanghyuk Chun</strong>, Gayoung Lee, Bado Lee, Seonghyeon Kim, Hwalsuk Lee</li>
                                <li><strong><em>ECCV 2020.</em></strong> <a href="media/papers/cha2020eccv_dmfont.pdf">paper</a> | <a href="https://github.com/clovaai/dmfont">code</a> | <a href="https://www.youtube.com/watch?v=VMrMJf21XEA">video</a> | <a href="media/bibtex/cha2020dmfont.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected">
                            <strong class="anchor-strong">Learning De-biased Representations with Biased Representations.</strong>
                            <ul>
                                <li>Hyojin Bahng, <strong>Sanghyuk Chun</strong>, Sangdoo Yun, Jaegul Choo, Seong Joon Oh</li>
                                <li><strong><em>ICML 2020.</em></strong> <a href="media/papers/bahng2020icml_rebias.pdf">paper</a> | <a href="https://github.com/clovaai/rebias">code</a> | <a href="https://twitter.com/SanghyukChun/status/1278357842362155008">tweet</a> | <a href="https://youtu.be/lkjMxZDGubA">video</a> | <a href="https://docs.google.com/presentation/d/1edTv6-gHs-HCF10f2gUCIXiWJ-nSZWg7dzp5q-eKeRk/edit?usp=sharing">slide</a> | <a href="media/bibtex/bahng2020rebias.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">Toward High-quality Few-shot Font Generation with Dual Memory.</strong> <span class="badge bg-danger">Oral presentation</span> <span class="badge bg-warning">The best paper runner-up award</span>
                            <ul>
                                <li>Junbum Cha, <strong>Sanghyuk Chun</strong>, Gayoung Lee, Bado Lee, Seonghyeon Kim, Hwalsuk Lee</li>
                                <li><strong><em>CVPR Workshop 2020.</em></strong> <a href="media/papers/cha2020cvprws.pdf">paper</a> | <a href="media/bibtex/cha2020cvprw.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Evaluating Weakly Supervised Object Localization Methods Right.</strong>
                            <ul>
                                <li>Junsuk Choe<sup>&#10059;</sup>, Seong Joon Oh<sup>&#10059;</sup>, Seongho Lee, <strong>Sanghyuk Chun</strong>, Zeynep Akata, Hyunjung Shim</li>
                                <li><strong><em>CVPR 2020.</em></strong> <a href="media/papers/choe2020cvpr_wsoleval.pdf">paper</a> | <a href="https://github.com/ClovaAI/wsolevaluation">code and dataset</a> | <a href="https://twitter.com/SanghyukChun/status/1271329234217099264">tweet</a> | <a href="media/slides/CVPR20_slide_evaluating_wsol_methods_right.pdf">slide</a> | <a href="https://www.youtube.com/watch?v=Vy1NcMxUi_Y">video on CVPR</a> | <a href="https://youtu.be/D_dEkeb-fto">video on ECCV tutorial</a> | <a href="media/bibtex/choe2020wsoleval.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Data-driven Harmonic Filters for Audio Representation Learning.</strong>
                            <ul>
                                <li>Minz Won, <strong>Sanghyuk Chun</strong>, Oriol Nieto, Xavier Serra</li>
                                <li><strong><em>ICASSP 2020.</em></strong> <a href="media/papers/won2020icassp_hcnn.pdf">paper</a> | <a href="https://github.com/minzwon/sota-music-tagging-models">code and pretrained models</a> | <a href="https://youtu.be/BXHEYb_Axus">video</a> | <a href="media/bibtex/won2020harmonic.txt">bibtex</a></li>
                            </ul>
                        </li>
                    </ul>
                    <div class="card-header no-border">
                        <h4 class="float-left mb-0" id="papers-2019">2019</h4>
                        <!--<span class="float-left"><strong> &nbsp; &nbsp; 2 conference papers, 4 workshop papers (2 ICCV, 1 ICLR WS, 2 ICML WS, 1 ISMIR LBD)</strong></span>-->
                        <div class="clearfix"></div>
                    </div>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">Neural Approximation of Auto-Regressive Process through Confidence Guided Sampling.</strong>
                            <ul>
                                <li>YoungJoon Yoo, <strong>Sanghyuk Chun</strong>, Sangdoo Yun, Jung-Woo Ha, Jaejun Yoo</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/yoo2019nara.pdf">paper</a> | <a href="media/bibtex/yoo2019nara.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">Toward Interpretable Music Tagging with Self-attention.</strong>
                            <ul>
                                <li>Minz Won, <strong>Sanghyuk Chun</strong>, Xavier Serra</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/won2019mtsa.pdf">paper</a> | <a href="https://github.com/minzwon/sota-music-tagging-models">code and pretrained models</a> | <a href="media/bibtex/won2019attention.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference li-selected-two">
                            <strong class="anchor-strong">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.</strong> <span class="badge bg-danger">Oral presentation</span>
                            <ul>
                                <li>Sangdoo Yun, Dongyoon Han, Seong Joon Oh, <strong>Sanghyuk Chun</strong>, Junsuk Choe, Youngjoon Yoo</li>
                                <li><strong><em>ICCV 2019.</em></strong> <a href="media/papers/yun2019iccv_cutmix.pdf">paper</a> | <a href="https://github.com/ClovaAI/CutMix-PyTorch">code and pretrained models</a> | <a href="https://clova-ai.blog/2019/07/15/cutmix-regularization-strategy-to-train-strong-classifiers-with-localizable-features/">blog</a> | <a href="media/bibtex/yun2019cutmix.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">Photorealistic Style Transfer via Wavelet Transforms.</strong>
                            <ul>
                                <li>Jaejun Yoo<sup>&#10059;</sup>, Youngjung Uh<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Byungkyu Kang, Jung-Woo Ha</li>
                                <li><strong><em>ICCV 2019.</em></strong> <a href="media/papers/yoo2019iccv_wct2.pdf">paper</a> | <a href="https://github.com/ClovaAI/WCT2">code and model weights</a> | <a href="https://youtu.be/o-AgHt1VA30">video</a> | <a href="https://clova-ai.blog/2019/08/06/photorealistic-style-transfer-via-wavelet-transforms-iccv-2019/">blog</a> | <a href="https://sanghyukchun.github.io/home/media/bibtex/yoo2019wct2.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">Automatic Music Tagging with Harmonic CNN.</strong>
                            <ul>
                                <li>Minz Won, <strong>Sanghyuk Chun</strong>, Oriol Nieto, Xavier Serra</li>
                                <li><strong><em>ISMIR LBD 2019.</em></strong> <a href="media/papers/won2019ismirlbd.pdf">paper</a> | <a href="https://github.com/minzwon/fast-harmonic-cnn">code</a> | <a href="media/bibtex/won2019lbd.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">An Empirical Evaluation on Robustness and Uncertainty of Regularization methods.</strong>
                            <ul>
                                <li><strong>Sanghyuk Chun</strong>, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, Youngjoon Yoo</li>
                                <li><strong><em>ICML Workshop 2019.</em></strong> <a href="media/papers/chun2019icmlws.pdf">paper</a> | <a href="media/bibtex/chun2019icmlw.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">Visualizing and Understanding Self-attention based Music Tagging.</strong> <span class="badge bg-danger">Oral presentation</span>
                            <ul>
                                <li>Minz Won, <strong>Sanghyuk Chun</strong>, Xavier Serra</li>
                                <li><strong><em>ICML Workshop 2019.</em></strong> <a href="media/papers/won2019icmlws.pdf">paper</a> | <a href="https://github.com/minzwon/sota-music-tagging-models">code</a> | <a href="https://slideslive.com/38917230/visualizing-and-understanding-selfattention-based-music-tagging">talk video</a> | <a href="media/bibtex/won2019icmlw.txt">bibtex</a></li>
                            </ul>
                        </li>

                        <li class="list-group-item li-workshop">
                            <strong class="anchor-strong">Where To Be Adversarial Perturbations Added? Investigating and Manipulating Pixel Robustness Using Input Gradients.</strong>
                            <ul>
                                <li>Jisung Hwang<sup>&#10059;</sup>, Younghoon Kim<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong><sup>&#10059;</sup>, Jaejun Yoo, Ji-Hoon Kim, Dongyoon Han</li>
                                <li><strong><em>ICLR Workshop 2019.</em></strong> <a href="media/papers/hwang2019iclrws.pdf">paper</a> | <a href="media/bibtex/hwang2019prm.txt">bibtex</a></li>
                            </ul>
                        </li>
                    </ul>
                    <h4 class="card-header no-border" id="papers-2018">~ 2018</h4>
                    <ul class="list-group list-group-flush list-no-border">
                        <li class="list-group-item li-arxiv">
                            <strong class="anchor-strong">Multi-Domain Processing via Hybrid Denoising Networks for Speech Enhancement.</strong>
                            <ul>
                                <li>Jang-Hyun Kim<sup>&#10059;</sup>, Jaejun Yoo<sup>&#10059;</sup>, <strong>Sanghyuk Chun</strong>, Adrian Kim, Jung-Woo Ha</li>
                                <li><strong><em>preprint.</em></strong> <a href="media/papers/kim2018mdphd.pdf">paper</a> | <a href="https://mdphdnet.github.io/">project page</a> | <a href="media/bibtex/kim2018mdphd.txt">bibtex</a></li>
                            </ul>
                        </li>
                        <li class="list-group-item li-conference">
                            <strong class="anchor-strong">A Study on Intelligent Personalized Push Notification with User History.</strong>
                            <ul>
                                <li>Hyunjong Lee, Youngin Jo, <strong>Sanghyuk Chun</strong>, Kwangseob Kim</li>
                                <li><strong><em>Big Data 2017.</em></strong> <a href="https://ieeexplore.ieee.org/abstract/document/8258081/">paper</a> | <a href="media/bibtex/lee2017ippn.txt">bibtex</a></li>
                            </ul>
                        </li>
                        <li class="list-group-item li-others li-selected">
                            <strong class="anchor-strong">Scalable Iterative Algorithm for Robust Subspace Clustering: Convergence and Initialization.</strong>
                            <ul>
                                <li>Master's Thesis, Korea Advanced Institute of Science and Technology, 2016 (advised by <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>) <a href="media/papers/chun2016scsi.pdf">paper</a> | <a href="http://github.com/SanghyukChun/SC_SI">code</a></li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <hr>

                <h3 class="mt-5" id="activities">Academic Activities</h3>
                <div class="card mt-4 mb-4">
                    <h4 class="card-header">Professional Service</h4>
                    <div class="card-body">
                        <ul class="list-unstyled">
                            <li><strong>Journal Action Editor:</strong>
                                <ul>
                                    <li><a href="https://jmlr.org/tmlr/editorial-board.html">Transactions on Machine Learning Research (TMLR)</a></li>
                                </ul>
                            </li>
                        </ul>
                        <ul class="list-unstyled">
                            <li><strong>Conference Area Chair:</strong>
                                <ul>
                                    <li>ICLR 2025</li>
                                    <li>AISTATS 2025</li>
                                    <li>NeurIPS 2024-2025</li>
                                    <li>NeurIPS Dataset and Benchmark (D&amp;B) track 2023-2024</li>
                                    <li>ICML 2025</li>
                                </ul>
                            </li>
                        </ul>
                        <ul class="list-unstyled">
                            <li><strong>Tutorial / Workshop / Social Organizer:</strong>
                                <ul class="list-group list-group-flush list-no-border">
                                    <li class="list-group-item li-tutorial">
                                        <strong class="anchor-strong">FAccT 2022 Translation/Dialogue Tutorial: "Shortcut learning in Machine Learning: Challenges, Analysis, Solutions"</strong>
                                        <ul>
                                            <li>Co-organized by <strong>Sanghyuk Chun</strong>, Kyungwoo Song, Yonghan Jung</li>
                                            <li><a href="https://sites.google.com/view/facct22-shortcut-learning/home">homepage</a> | <a href="https://docs.google.com/presentation/d/1UP-unGwtOhiO5rihMzNaXSCtn9fF7J5gLqmsY6jLve0/edit?usp=sharing">slide</a> | <a href="https://www.youtube.com/watch?v=8830gv2mIss">video</a></li>
                                        </ul>
                                    </li>
                                    <li class="list-group-item li-workshop-orig">
                                        <strong class="anchor-strong">NeurIPS 2021 Workshop on <a href="https://sites.google.com/view/imagenet-workshop/home">ImageNet: Past, Present, and Future</a></strong>
                                        <ul>
                                            <li>Co-organized by Zeynep Akata, Lucas Beyer, <strong>Sanghyuk Chun</strong>, Almut Sophia Koepke, Diane Larlus, Seong Joon Oh, Rafael Sampaio de Rezende, Sangdoo Yun, Xiaohua Zhai</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                            <ul>
                                <li><a href="https://iclr.cc/Conferences/2021/Schedule?showEvent=4408">ICLR 2021 Social: ML in Korea</a>, technical chair and session chair</li>
                                <li><a href="https://iclr.cc/Conferences/2022/Schedule?showEvent=8740">ICLR 2022 Social: ML in Korea</a>, a main organizer</li>
                            </ul>
                        </ul>
                        <ul class="list-unstyled">
                            <li><strong>Reviewer:</strong>
                                <ul>
                                    <li>Conference: ICML 2021-2024, NeurIPS 2020-2023 <strong class="text-danger">(NeurIPS 2023 Top reviewer)</strong>, ICLR 2021-2024, AAAI 2021, CVPR 2020-2025 <strong class="text-danger">(CVPR 2020, 2021, 2022 Outstanding reviewer)</strong>, ICCV/ECCV 2021-2025, WACV 2021, ACCV 2020, CHI 2023</li>
                                    <li>Journal: <a href="https://jmlr.org/tmlr/">Transactions on Machine Learning Research (TMLR)</a> <strong><a class="text-danger" href="https://jmlr.org/tmlr/expert-reviewers.html">(TMLR 2023 Expert Reviewer)</a></strong>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</a>, <a href="https://www.springer.com/journal/11263">International Journal of Computer Vision (IJCV)</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing (TIP)</a></li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="card mb-4">
                    <h4 class="card-header">Awards</h4>
                    <div class="card-body">
                        <ul class="pl15 mb-0">
                            <li><a href="https://neurips.cc/Conferences/2023/ProgramCommittee#top-reivewers">Top reviewer, NeurIPS 2023</a></li>
                            <li><a href="https://jmlr.org/tmlr/expert-reviewers.html">TMLR Expert Reviewer (2023)</a></li>
                            <li><a href="https://cvpr2022.thecvf.com/outstanding-reviewers">Outstanding reviewer award, CVPR 2022</a></li>
                            <li><a href="https://cvpr2021.thecvf.com/node/184">Outstanding reviewer award, CVPR 2021</a></li>
                            <li><a href="https://cvpr2020.thecvf.com/reviewer-acknowledgements#outstanding-reviewers">Outstanding reviewer award, CVPR 2020</a></li>
                            <li><a href="https://ai4cc.net/2020/">Best paper runner-up award, AI for Content Creation Workshop at CVPR 2020</a></li>
                        </ul>
                    </div>
                </div>

                <div class="card mb-4">
                    <h4 id="talks" class="card-header">Talks</h4>
                    <div class="card-body">
                        <ul class="list-unstyled mb-0">
                            <li>"Probabilistic Language-Image Pre-training", POSTECH AI Day (2024) <a href="https://docs.google.com/presentation/d/1BEHEphXxdg0TjUsI3Cv8Xr3kLX6sbAlGytDEiN5iW7s/edit?usp=sharing">[slide]</a></li>
                            <li>"Realistic challenges and limitations of AI", SKKU (2024). <a href="https://docs.google.com/presentation/d/1s_7f3Uu6CtYrucFYQLhCJyV8l3Nhz7QZebyxpi5IwLs/edit?usp=sharing">[slide]</a></li>
                            <li>"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion", <a href="https://soict.hust.edu.vn/summer-school">HUST AI Summer School on "Generative AI"</a> (2024). <a href="https://docs.google.com/presentation/d/1GEVu5aZUxeJg3B6AU4iORlfQ1qCA5tIQjePkk0q6c1c/edit?usp=sharing">[slide]</a></li>
                            <li>"Font Generation", Dankook University (2024).</li>
                            <li>"Probabilistic Image-Text Representations", <a href="https://www.theieie.org/events/?part=03&c_id=872">IEIE AI Signal Processing Society Winter School</a> and UNIST AIGS Seminar (2024). <a href="https://docs.google.com/presentation/d/1yelrDSN11rnChAk-gtU2YzSNX49XPHFIgROzj_lRA4Q/edit?usp=sharing">[slide]</a></li>
                        </ul>
                            <details>
                                <summary><strong>See older talks</strong></summary>
                                <ul class="list-unstyled">
                            <li>"Probabilistic Image-Text Representation", <a href="https://soict.hust.edu.vn/summer-school">HUST AI Summer School on "Modern Machine Learning: Foundations and Applications"</a> and Dankook University (2023). <a href="https://docs.google.com/presentation/d/1IB-2A8w--jjQ9TAp1Xn8ANkfq_NyXAKaHtX3dh2e9_4/edit?usp=sharing">[slide]</a></li>
                            <li>"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion", Yonsei University (2023). <a href="https://docs.google.com/presentation/d/1VTJlrHqnLAcQP3aHydnlFXNeZpsPMGa9-L-Oaigi_6M/edit?usp=sharing">[slide]</a></li>
                            <li>"Probabilistic Image-Text Representations", Sogang University (2023). <a href="https://docs.google.com/presentation/d/1hLCAGuY3HuJYzo20Puugw79aFSAIz8BMej-WJxCIsJk/edit?usp=sharing">[slide]</a></li>
                            <li>"ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO", NAVER and Sogang University (2022). <a href="https://docs.google.com/presentation/d/1OKaWPlNblepiXF57oWs2miGgYb5kuu1qxNqV_-hDddU/edit?usp=sharing">[slide]</a></li>
                            <li>"Towards Reliable Machine Learning: Challenges, Examples, Solutions", UNIST AIGS (2022). <a href="https://docs.google.com/presentation/d/1SK2XwkQX5TPbkObDGnGgY4LWg6K-0dUdqyuosyVv2EI/edit?usp=sharing">[slide]</a></li>
                            <li>"Tutorial on Shortcut learning in Machine Learning: Challenges, Analysis, Solutions" at <a href="https://facctconference.org/2022/schedule.html">FAccT 2022</a>. [ <a href="https://sites.google.com/view/facct22-shortcut-learning/home">tutorial homepage</a> | <a href="https://docs.google.com/presentation/d/1UP-unGwtOhiO5rihMzNaXSCtn9fF7J5gLqmsY6jLve0/edit?usp=sharing">slide</a> | <a href="https://www.youtube.com/watch?v=8830gv2mIss">video</a> ]</li>
                            <li>"Towards Reliable Machine Learning", KAIST and SNU (2022). <a href="https://docs.google.com/presentation/d/1Z1XNVl6LfslCyTsERQQRzTb6qMpuyc-8wkMIYuu7IAY/edit?usp=sharing">[slide]</a></li>
                            <!--<li>"Research in NAVER AI", <a href="http://www.ipiu.or.kr/">IPIU 2022</a> [slide TBA]</li>-->
                            <li>"Shortcut learning in Machine Learning: Challenges, Examples, Solutions", <a href="https://sites.google.com/view/pair-ml-winter-seminar-2022/home">POSTECH AI Research (PAIR) ML Winter Seminar 2022</a>. <a href="https://docs.google.com/presentation/d/1LGtjxaXwkk_Z6OaJFUEjputi6eEtkq3h1sXB2yx7gPk/edit?usp=sharing">[slide]</a></li>
                            <li>"Realistic challenges and limitations of AI", University of Seoul (2021). <a href="https://docs.google.com/presentation/d/1FOIMm4bYWN6b80_H0El1N4PzGu-2gfNqfR6bgjU8-58/edit?usp=sharing">[slide]</a></li>
                            <li>"Mitigating dataset biases in Real-world ML applications", NAVER and NAVER Labs Europe (2021). <a href="https://docs.google.com/presentation/d/1jyrfZfSEwbgzSuVKUURfmIEo3baksOL2GxSrBSzR4qo/edit">[slide]</a></li>
                            <li>"Limits and Challenges in Deep Learning Optimizers", UNIST (2021). <a href="https://docs.google.com/presentation/d/1NNftqS6BcCPd52tv8gWEjB34retYhP0FToOFBd9ewkQ/edit">[slide]</a></li>
                            <li>"Towards better cross-modal learning by Probabilistic embedding and AdamP optimizer", UAB CVC (2021). <a href="http://www.cvc.uab.es/?p=7778">[info]</a> <a href="https://docs.google.com/presentation/d/1dGQUqud3iMld-UgMMlRQuqA7JhzndMfzFoKaFAwZ58I/edit?usp=sharing">[slide]</a></li>
                            <li>"AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights", KSIAM (2021). <a href="https://docs.google.com/presentation/d/1s9zgQ22WFnhEj6POL_0ecrTiED__qmL9XgL0Nv3zNP4/edit?usp=sharing">[slide]</a></li>
                            <li>"Towards Few-shot Font Generation", Seoul University and NAVER (2021). <a href="https://docs.google.com/presentation/d/13WoYS9r9C751s3nZ2yF5WdestRXh3V7LFQDOJyCrt9s/edit?usp=sharing">[slide]</a></li>
                            <li>"Probabilistic Embeddings for Cross-modal Retrieval", SNU AI Institute (AIIS) Retreat. <a href="https://docs.google.com/presentation/d/1GxyiehfrJRgN8Bv7OCoFaW0zkJT2-mdbKKwgeRal8do/edit?usp=sharing">[slide]</a></li>
                            <li>"Learning De-biased Representations with Biased Representations", NAVER (2020). <a href="https://docs.google.com/presentation/d/1edTv6-gHs-HCF10f2gUCIXiWJ-nSZWg7dzp5q-eKeRk/edit?usp=sharing">[slide]</a></li>
                            <li>"Reliable Machine Learning in NAVER AI", Yonsei University (2020). <a href="https://docs.google.com/presentation/d/1CMeF-AbbKBX-NzzcnEe6YiZjWMdzp3ZknPnD2B9mSoE/edit?usp=sharing">[slide]</a></li>
                            <li>"Toward Reliable Machine Learning", <a href="https://www.omnious.com/">omnious</a> and <a href="https://www.nota.ai/">nota</a> (2020). <a href="https://docs.google.com/presentation/d/1XCALHYzk9p_EjON9qckrylGClWZVBpzrFWzrd3a-qhs/edit?usp=sharing">[slide]</a></li>
                            <li>"Reliable Machine Learning", NAVER CVPR 2020 sponser event. <a href="https://europe.naverlabs.com/naver-interactive-sessions-at-cvpr-2020/">[program]</a> <a href="https://docs.google.com/presentation/d/1bzFN6SZman387_etABgP2iuDBUDNbvBbepGRGDKvf64/edit?usp=sharing">[slide]</a> <a href="https://www.youtube.com/watch?v=Z_e_XWJWeJ8">[video]</a></li>
                            <li>"Neural Architectures for Music Representation Learning", NAVER (2020). <a href="media/slides/2020_May_music_architectures.pdf">[slide]</a></li>
                            <li>"Learning generalizable representations with CutMix and ReBias", NAVER Labs Europe (2019).</li>
                            <li>"An empirical evaluation on the generalizability of regularization methods", <a href="https://europe.naverlabs.com/icml2019-expo-workshop-machine-learning-naver/">ICML 2019 Expo Talk: Recent Work on Machine Learning at NAVER</a>. <a href="media/slides/190609_expotalk.pdf">[slide]</a></li>
                            <li>"Recent works on deep learning robustness in Clova AI", <a href="https://iclr.cc/ExpoConferences/2019/schedule?talk_id=3">ICLR 2019 Expo Talk: Representation Learning to Rich AI Services in NAVER and LINE</a>.</li>
                            <li>"Recommendation system in the real world", <a href="https://deepest.ai/">Deepest</a> Summer School 2018. <a href="media/slides/180825_recsys_summer_school.pdf">[slide]</a></li>
                            </ul>
                            </details>
                    </div>
                </div>

                <details class="mb-3">
                    <summary class="mb-3"><span style="font-size: 1.25rem; color: #303030;">Code and Data</span></summary>
                <div class="card mt-4 mb-4">
                    <h4 class="card-header">Datasets</h4>
                    <div class="card-body">
                        <ul>
                            <li><a href="https://github.com/ClovaAI/wsolevaluation">CUB v2 and OpenImages 30k</a> [C5] [J2]: These datasets were designed and collected for the WSOL evaluation project. We newly collected bird images for CUB v2 (200 classes) and re-organize OpenImages V5 for OpenImages 30k (100 classes)</li>
                            <li><a href="https://github.com/clovaai/rebias">Biased MNIST, 9-Class ImageNet and ImageNet cluster labels</a> [C6]: We proposed these datasets for measuring how ML models can be generalized to bias shift. Biased MNIST is a synthetic dataset based on MNIST, while each image has background colors which highly correlates with the labels (controllable). ImageNet-9 contains 9 super-classes (dog, cat, frog, turtle, bird, monkey, fish, crab, insect) and 57k training samples. We also proposes "unbiased accuracy" by using cluster labels (K=9), which empicially matches to Shell, Grass, Close-up, Eye, Human, Sand and Mammal.</li>
                            <li><a href="https://github.com/naver-ai/pcme">CUB Caption</a> [C11]: CUB was not majorly used for image-text matching (ITM) retrieval. However, while doing the PCME project, we proposed to use the CUB dataset as a ITM benchmark for measuring the impact of many-to-many correspondence; if an image and a description are in the same class, then we treat it as "positive" otherwise "negative". We carefully devide the training-validation (150 classes) and test splits (50 classes) following Xian et al. 2017.</li>
                            <li><a href="https://github.com/naver-ai/eccv-caption">ECCV Caption</a> [C21]: Although CUB Caption can evaluate the impact of many-to-many correspondence in ITM, this dataset is still "synthetic". For a more practical usage, we proposed the ECCV Caption benchmark, by correcting the false negatives (FNs) in the MS-COCO Caption dataset. By machine and human annotators, we collected x8.47 positive images and x3.58 positive captions compared to the original COCO Caption.</li>
                            <li><a href="https://github.com/pseulki/rococo">RoCOCO</a> [W9]: Is an ITM model robust to a malicious manipulation on captions or images? Here, we investigated the impact of altered captions (same concept, different concept, rand voca attack, and dangerous word) and mixed images. Even state-of-the-art ITM models showed substantial performance degradations on our benchmark.</li>
                        </ul>
                    </div>
                </div>

                <div class="card mt-4 mb-4">
                    <h4 class="card-header">Softwares</h4>
                    <div class="card-body">
                        <ul class="list-group list-group-flush list-no-border">
                            <li class="list-group-item li-others">
                                <strong class="anchor-strong">Few-shot Font Generation Benchmark.</strong>
                                <ul>
                                    <li>Song Park, <strong>Sanghyuk Chun</strong></li>
                                <li><strong><em>open source.</em></strong> <a href="https://github.com/clovaai/fewshot-font-generation">code</a></li>
                                </ul>
                            </li>
                            <li class="list-group-item li-others">
                                <strong class="anchor-strong">Graphit: A Unified Framework for Diverse Image Editing Tasks.</strong>
                                <ul>
                                    <li>Geonmo Gu, <strong>Sanghyuk Chun</strong>, Wonjae Kim, HeeJae Jun, Sangdoo Yun, Yoohoon Kang</li>
                                    <li><strong><em>open source.</em></strong> <a href="https://github.com/navervision/Graphit">code</a> | <a href="https://huggingface.co/spaces/navervision/Graphit-SD">demo</a></li>
                                </ul>
                            </li>
                        </ul>
                        <ul>
                            <li>You can install my softwares [C9] [C21] via pip by following commands:</li>
                            <pre>
pip install adamp
pip install eccv_caption</pre>
                            <li>Check the following HuggingFace hub links for the official weights [C25] [C34]:</li>
                            <ul>
                                <li>PCME++ [C25]: <a href="https://huggingface.co/collections/SanghyukChun/pcme-6652e86eb6fc2144ec26bf09">https://huggingface.co/collections/SanghyukChun/pcme-6652e86eb6fc2144ec26bf09</a></li>
                                <li>ProLIP [C34]: <a href="https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291">https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291</a></li>
                            </ul>
                        </ul>
                    </div>
                </div>

                <hr>

                </details>

                <details class="mb-3">
                    <summary class="mb-3"><span style="font-size: 1.25rem; color: #303030;">Industry Experience</span></summary>

                <div class="card mb-4">
                    <h4 class="card-header" id="experience-in-naver">NAVER AI Lab (2018 ~ Now)</h4>
                    <ul class="list-group list-group-flush">
                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_hangul_event.png" width="100%" alt="Hangul" class="figure-img img-fluid rounded">
                                    </figure>
                                    <figure class="figure">
                                        <img src="assets/img/project_hangul_event2.png" width="100%" alt="Hangul" class="figure-img img-fluid rounded">
                                    </figure>
                                    <figure class="figure">
                                        <img src="assets/img/dmfont-teaser.gif" width="100%" alt="DM-Font teasor" class="figure-img img-fluid rounded">
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Hangul Handwriting Font Generation</strong>
                                    <p>Distributed at 2019 Hangul's day (한글날), <a href="https://clova.ai/handwriting/list.html">[Full font list]</a></p>
                                    <ul class="list-unstyled">
                                        <li>Hangul (Korean alphabet, 한글) originally consists of only 24 sub-letters (ㄱ, ㅋ, ㄴ, ㄷ, ㅌ, ㅁ, ㅂ, ㅍ, ㄹ, ㅅ, ㅈ, ㅊ, ㅇ, ㅎ, ㅡ, ㅣ, ㅗ, ㅏ, ㅜ, ㅓ, ㅛ, ㅑ, ㅠ, ㅕ), but by combining them, there exist 11,172 valid characters in Hangul. For example, "한" is a combination of ㅎ, ㅏ, and ㄴ, and "쐰" is a combination of ㅅ, ㅅ, ㅗ, ㅣ, and ㄴ. It makes generating a new Hangul font be very expensive and time-consuming. Meanwhile, <a href="https://hangeul.naver.com/2014/history">since 2008</a>, Naver has distributed Korean fonts for free (named <a href="https://hangeul.naver.com/2017/nanum">Nanum fonts, 나눔 글꼴</a>).</li>
                                        <li>In 2019, we developed a technology for fully-personalized Hangul generation only with 152 characters. We opened <a href="https://clova.ai/handwriting">an event page</a> where users can submit their own handwriting. The full generated font list can be found in <a href="https://clova.ai/handwriting/list.html">[this link]</a>. Details for the generation technique used for the service was presented in Deview 2019 <a href="https://deview.kr/2019/schedule/294">[Link]</a>.</li>
                                        <li>This work was also extended to the few-shot generation based on the compositionality. See the papers in AI for Content Creation Workshop (AICCW) at CVPR 2020 (short paper) <a href="media/papers/cha2020cvprws.pdf">[Link]</a>, ECCV 2020 (full paper) <a href="media/papers/cha2020eccv_dmfont.pdf">[Link]</a>, AAAI 2021 <a href="media/papers/park2021aaai_lffont.pdf">[Link]</a>, ICCV 2021 <a href="media/papers/park2021iccv_mxfont.pdf">[Link]</a>, and journal extension <a href="media/papers/park2021lffont_extension.pdf">[Link]</a>.</li>
                                        <li class="text-danger"><strong>[BONUS] You can play with my handwriting <a href="handwriting.html">here</a></strong></li>
                                    </ul>
                                </div>
                            </div>
                        </li>

                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_sticker_recommendation_example.png" width="100%" alt="example sticker" class="figure-img img-fluid rounded">
                                        <figcaption class="figure-caption">Example emoji from <a href="https://store.line.me">LINE sticker shop</a>.<figcaption>
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Emoji Recommendation (LINE Timeline)</strong>
                                    <p>Deployed in Jan. 2019</p>
                                    <ul class="list-unstyled">
                                        <li><a href="https://linecorp.com/en/">LINE</a> is a major messenger player in east asia (Japan, Taiwan, Thailand, Indonesia, and Korea). In the application, users can buy and use numerous emoijs a.k.a. <a href="https://store.line.me">LINE Sticker</a>.</li>
                                        <li>In this project, we recommended emojis to users based on their profile picture (<strong>cross-domain recommendation</strong>).</li>
                                        <li>I developed and researched the entire pipeline of the cross-domain recommendation system and operation tools.</li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                        <li class="list-group-item">
                            <div class="row m-3">
                                <strong>Mentees / Short-term post-doctoral collaborators / Internship students</strong>
                                <div class="card-body">
                                    <p>Topics: <span class="badge bg-success">Reliable ML</span> <span class="badge bg-danger">Vision-Language</span> <span class="badge bg-warning">Modality-specific tasks</span> <span class="badge bg-info">Generative models</span> <span class="badge bg-dark">Other topics</a></p>
                                    <ul class="list-unstyled mb-0">
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://jnwnlee.github.io/">Junwon Lee</a> (KAIST, 2025) <span class="text-danger"> -- AVL representation learning</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sehyunkwon.github.io/">Sehyun Kwon</a> (Seoul National University, 2024) <span class="text-danger"> -- VL representation learning</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://bellos1203.github.io/">Jaeyoo Park</a> (Seoul National University, 2024) <span class="text-danger"> -- VL representation learning</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://park-jungin.github.io/">Jungin Park</a> (Visiting researcher, 2024) <span class="text-danger"> -- VL representation learning</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://eugene6923.github.io/">Yujin Jeong</a> (Korea University, 2024) <a href="#read-watch-and-scream-sound-generation-from-text-and-video">[C33/W10]</a> <span class="text-danger"> -- AVL representation learning</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill bg-success text-success">_</span> <a href="https://scholar.google.co.kr/citations?user=D9U_ohsAAAAJ&hl=en">Heesun Bae</a> (KAIST, 2023) <span class="text-danger"> -- VL representation learning under noisy environment</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://jbeomlee93.github.io/">Jungbeom Lee</a> (Visiting researcher, 2023) <a href="#toward-interactive-regional-understanding-in-vision-large-langua">[C28]</a> <span class="text-danger"> -- VL representation learning</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sites.google.com/snu.ac.kr/eunjikim">Eunji Kim</a> (Seoul National University, 2022) <span class="text-success"> -- XAI + Probabilistic Machine</span> (the internship project is published at ICML 2023 <a href="https://arxiv.org/abs/2306.01574">[paper]</a>)</li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://j-h-hwang.github.io/">Jaehui Hwang</a> (Yonsei University, 2022) <a href="#similarity-of-neural-architectures-based-on-input-gradient-trans">[C30]</a> <span class="text-success"> -- Adversarial robustness and XAI</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://chanwoo-park-official.github.io/">Chanwoo Park</a> (Seoul National University, 2021-2022) <a href="#a-unified-analysis-of-mixed-sample-data-augmentation-a-loss-func">[C22]</a> <span class="text-dark"> -- Deep learning theory</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://mks0601.github.io/">Gyeongsik Moon</a> (Visiting researcher, 2022) <a href="#three-recipes-for-better-3d-pseudo-gts-of-3d-human-mesh-estimati">[W7]</a> <span class="text-dark"> -- Semi-supervised learning for 3D Human Mesh Estimation</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://hongsukchoi.github.io/">Hongsuk Choi</a> (Visiting researcher, 2022) <a href="#three-recipes-for-better-3d-pseudo-gts-of-3d-human-mesh-estimati">[W7]</a> <span class="text-dark"> -- Semi-supervised learning for 3D Human Mesh Estimation</span></li>
                                        <li><span class="badge round-pill bg-danger text-danger">_</span> <span class="badge round-pill bg-success text-success">_</span> <a href="https://scholar.google.com/citations?user=6Wh4hxcAAAAJ&hl=en">Seulki Park</a> (Seoul National University, 2022) <a href="#rococo-robust-benchmark-of-ms-coco-to-stress-test-robustness-of">[W9]</a> <span class="text-danger"> -- VL robustness benchmark</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://scholar.google.com/citations?user=nS24h74AAAAJ&hl=en">Saehyung Lee</a> (Seoul National University, 2021-2022) <a href="#dataset-condensation-with-contrastive-signals">[C19]</a> <span class="text-dark"> -- Data condensation</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sites.google.com/view/sangwon-jung">Sangwon Jung</a> (Seoul National University, 2021-2023) <a href="#learning-fair-classifiers-with-partially-annotated-group-labels">[C18]</a><span class="text-success"> <a href="#dataset-condensation-with-contrastive-signals">[C19]</a> -- Fairness with not enough group labels, group fairness</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://lucascimeca.com/">Luca Scimeca</a> (A short-term post-doctoral collaborator, 2021) <a href="#which-shortcut-cues-will-dnns-choose-a-study-from-the-parameter">[C16]</a> <a href="#neural-hybrid-automata-learning-dynamics-with-multiple-modes-and">[C14]</a><span class="text-success"> -- Understanding shortcut learning phenomenon in feature space</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://zymrael.github.io/">Michael Poli</a> (KAIST, 2021) <a href="#neural-hybrid-automata-learning-dynamics-with-multiple-modes-and">[C14]</a> <a href="#which-shortcut-cues-will-dnns-choose-a-study-from-the-parameter">[C16]</a><span class="text-dark"> -- Neural hybrid automata</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sites.google.com/view/hyemikim/home">Hyemi Kim</a> (KAIST, 2021)<span class="text-success"> -- Test-time training for robust prediction</span></li>
                                        <li><span class="badge round-pill bg-dark text-dark">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sites.google.com/view/istar-jun">Jun Seo</a> (KAIST, 2021)<span class="text-dark"> -- Self-supervised learning</span></li>
                                        <li><span class="badge round-pill bg-info text-info">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://8uos.github.io/">Song Park</a> (Yonsei University, 2020-2021) <a href="#few-shot-font-generation-with-localized-style-representations-an">[C8/W6]</a> <a href="#multiple-heads-are-better-than-one-few-shot-font-generation-with">[C12]</a> <a href="#styleaugment-learning-texture-de-biased-representations-by-style">[A4]</a> <a href="#few-shot-font-generation-with-weakly-supervised-localized-repres">[J2]</a><span class="text-info"> -- Few-shot font generation</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://hjbahng.github.io/">Hyojin Bahng</a> (Korea University, 2019) <a href="#learning-de-biased-representations-with-biased-representations">[C6]</a><span class="text-success"> -- De-biasing</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a> (Yonsei University, 2019) <a href="#evaluating-weakly-supervised-object-localization-methods-right">[C5]</a> <a href="#evaluation-for-weakly-supervised-object-localization-protocol-me">[J1]</a><span class="text-dark"> -- Reliable evaluation for WSOL</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://namangoyal.com/">Naman Goyal</a> (IIT RPR, 2019) <span class="text-success">-- Robust representation against shift</span></li>
                                        <li><span class="badge round-pill bg-warning text-warning">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://minzwon.github.io/">Minz Won</a> (Music Technology Group, Universitat Pompeu Fabra, 2018-2019) <a href="#visualizing-and-understanding-self-attention-based-music-tagging">[W2]</a> <a href="#automatic-music-tagging-with-harmonic-cnn">[W4]</a> <a href="#toward-interpretable-music-tagging-with-self-attention">[A2]</a> <a href="#data-driven-harmonic-filters-for-audio-representation-learning">[C4]</a><span class="text-warning"> -- Audio representation learning</span></li>
                                        <li><span class="badge round-pill bg-info text-info">_</span>  <span class="badge round-pill">&nbsp;&nbsp;</span> Byungkyu Kang (Yonsei University, 2018) <a href="#photorealistic-style-transfer-via-wavelet-transforms">[C2]</a><span class="text-info"> -- Image-to-image translation and style transfer</span></li>
                                        <li><span class="badge round-pill bg-warning text-warning">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> <a href="https://janghyun1230.github.io/">Jang-Hyun Kim</a> (Seoul National University, 2018) <a href="#multi-domain-processing-via-hybrid-denoising-networks-for-speech">[A1]</a><span class="text-warning"> -- Audio representation learning</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> Jisung Hwang (University of Chicago, 2018) <a href="#where-to-be-adversarial-perturbations-added-investigating-and-ma">[W1]</a> <span class="text-success"> -- Adversarial robustness</span></li>
                                        <li><span class="badge round-pill bg-success text-success">_</span> <span class="badge round-pill">&nbsp;&nbsp;</span> Younghoon Kim (Seoul National University, 2018) <a href="#where-to-be-adversarial-perturbations-added-investigating-and-ma">[W1]</a><span class="text-success"> -- Adversarial robustness</span></li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>

                <div class="card mb-4">
                    <h4 class="card-header" id="experience-in-kakao">Kakao Advanced Recommendation Technology (ART) team (2016 ~ 2018)</h4>
                    <ul class="list-group list-group-flush">
                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_kakao.png" width="100%" alt="Kakao" class="figure-img img-fluid rounded">
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Recommender Systems (Kakao services)</strong>
                                    <p>Feb. 2016 - Feb. 2018</p>
                                    <ul class="list-unstyled">
                                        <li>I developed and maintained a large-scale real-time recommender system (Toros <a href="https://archive.pycon.kr/2016apac/program/50">[PyCon Talk]</a> <a href="https://brunch.co.kr/@kakao-it/72">[AI Report]</a>) for various services in <a href="https://www.daum.net/">Daum</a> and <a href="https://www.kakaocorp.com/?lang=en">Kakao</a>. I mainly worked with content-based representation modeling (for textual, visual, and musical data), collaborative filtering modeling, user embedding, user clustering, and ranking system based on Multi-armed Bandit.</li>
                                        <li><strong>Textual domain:</strong> <a href="https://media.daum.net/">Daum News</a> similar article recommendation, <a href="https://brunch.co.kr/">Brunch</a> (blog service) similar post recommendation, <a href="http://cafe.daum.net/">Daum Cafe</a> (community service) hit item recommendation.</li>
                                        <li><strong>Visual domain:</strong> <a href="http://webtoon.daum.net/">Daum Webtoon</a> and <a href="https://page.kakao.com/main">Kakao Page</a> similar item recommendation, video recommendation for a news article (cross-domain recommendation).</li>
                                        <li><strong>Audio domain:</strong> music recommendation for <a href="https://kakao.ai/product/kakaomini">Kakao Mini</a> (smart speaker), <a href="http://melon.com/">Melon</a> and <a href="https://www.kakaocorp.com/service/KakaoMusic?lang=en">Kakao Music</a>.</li>
                                        <li><strong>Online to offline:</strong> <a href="https://www.kakaocorp.com/service/KakaoHairShop?lang=en">Kakao Hairshop</a> style recommendation.</li>
                                    </ul>
                                </div>
                            </div>
                        </li>

                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_ippn.png" width="100%" alt="IPPN" class="figure-img img-fluid rounded">
                                        <figcaption class="figure-caption">System overview.<figcaption>
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Personalized Push Notification with User History (Daum, Kakao Page)</strong>
                                    <p>Deployed in 2017</p>
                                    <ul class="list-unstyled">
                                        <li>The mobile push service (or alert system) is widely-used in mobile applications to attain a high user retention rate. However, a freqeunt push notification makes a user feel fatigue, resulting on the application removal. Usually, the push notification system is a rule-based system, and managed by human labor. In this project, we researched and developed a personalized push notification system based on user activity and interests. The system has been applied to Daum an Kakao Page mobile applications. More details are in <a href="https://ieeexplore.ieee.org/abstract/document/8258081/">our paper</a>.</li>
                                    </ul>
                                </div>
                            </div>
                        </li>

                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_daumshopping.png" width="100%" alt="Daum Shopping" class="figure-img img-fluid rounded">
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Large-Scale Item Categorization in e-Commerce (Daum Shopping)</strong>
                                    <p>Deployed in 2017</p>
                                    <ul class="list-unstyled">
                                        <li>An accurate categorization helps users to search desired items in e-Commerce based on the category, e.g., clothes / shoes / sneakers. However, the categorization is usually performed based on rule-based systems or human labor, which leads to low coverage of categorized items. Even the automatic item categorization is difficult due to its web-scale data size, the highly unbalanced annotation distribution, and noisy labels. I developed a large-scale item categorization system for <a href="http://shopping.daum.net/">Daum Shopping</a> based on a deep network, from the operation tool to the categorization API.</li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>

                <div class="card mb-4">
                    <h4 class="card-header" id="experience-in-internship">Internship</h4>
                    <ul class="list-group list-group-flush">
                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_naverlabs.png" width="100%" alt="Naver Labs" class="figure-img img-fluid rounded">
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Research internship (Naver Labs)</strong>
                                    <p>Aug. 2015 - Dec. 2015</p>
                                    <ul class="list-unstyled">
                                        <li>During the internship, I implemented batch normalization (BN) to AlexNet, Inception v2 and VGG on ImageNet using Caffe. I also researched batch normalization for sequential models, e.g., RNN using Lua Torch.</li>
                                    </ul>
                                </div>
                            </div>
                        </li>

                        <li class="list-group-item">
                            <div class="row m-3">
                                <div class="col-md-2">
                                    <figure class="figure">
                                        <img src="assets/img/project_ium.png" width="100%" alt="IUM-SOCIUS" class="figure-img img-fluid rounded">
                                    </figure>
                                </div>
                                <div class="col-md-10">
                                    <strong>Software engineer (IUM-SOCIUS)</strong>
                                    <p>Jun. 2012 - Jan. 2013</p>
                                    <ul class="list-unstyled">
                                        <li>I worked as web developer at IUM-SOCIUS. During the internship, I developed and maintained internal batch services (JAVA spring batch), internal statistics service (Python Flask, MongoDB), internal admin tools (Python Django, MySQL), and main service systems (JAVA spring, Ruby on Rails, MariaDB).</li>
                                    </ul>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>
                </details>

                <details class="mb-3">
                    <summary class="mb-3"><span style="font-size: 1.25rem; color: #303030;">Education and Career</span></summary>
                <div class="card mb-4">
                    <div class="card-body">
                        <ul class="list-unstyled mb-0">
                            <li>M.S. (2014.03 - 2016.02), School of Electrical Engineering, KAIST</li>
                            <li>B.S. (2009.03 - 2014.02), School of Electrical Engineering and School of Management Science (double major), KAIST</li>
                        </ul>
                        <hr>
                        <ul class="pl15">
                            <li>Lead research scientist at NAVER AI Lab (Feb. 2018 - Now)</li>
                            <ul class="pl15">
                            <li>Leader of ML Research @NAVER AI Lab (Feb. 2022 - Now)</li>
                            <li>Tech leader at NAVER AI Lab (Oct. 2020 - Jan. 2022)</li>
                            <li>Visiting researcher at <a href="https://europe.naverlabs.com/">Naver Labs Europe</a> (Oct. 2019 - Dec. 2019)</li>
                            <li>Research scientist at NAVER CLOVA (Feb. 2018 - Sep. 2020)</li>
                            </ul>
                            <li>Research engineer at advanced recommendation team (ART) in <a href="https://www.kakaocorp.com/">Kakao</a> (Feb. 2016 - Feb. 2018)</li>
                            <li>Research internship at <a href="https://www.naverlabs.com/">Naver Labs</a> (Aug. 2015 - Dec. 2015)</li>
                            <li>Master's degree in Electrical Engineering from KAIST (advisor: <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>) (Mar. 2014 - Feb. 2016)</li>
                            <li>Undergraduate researcher at <a href="http://alinlab.kaist.ac.kr/">Algorithmic Intelligence Lab</a> in KAIST (Fall 2013)</li>
                            <li>Undergraduate researcher at <a href="http://www.ndsl.kaist.edu/">Networked and Distributed Computing System Lab</a> in KAIST (Summer 2013)</li>
                            <ul class="pl15">
                            <li>Participated as an undergraduate researcher for <a href="https://www.usenix.org/system/files/conference/atc15/atc15-paper-lee-jihyung.pdf">FloSIS: A Highly Scalable Network Flow Capture System for Fast Retrieval and Storage Efficiency</a> (presented at USENIC ATC 2015) (I developed the index system described in Section 4.)</li>
                            </ul>
                            <li>Software engineering internship at <a href="http://i-um.com/">IUM-SOCIUS</a> (Jun. 2012 - Jan. 2013)</li>
                        </ul>
                    </div>
                </div>
                </details>

                <!--<h3 class="mt-5" id="random">Random</h3>-->
                <!--<div class="row mb-4">-->
                    <!--<ul class="pl15">-->
                        <!--<li>I enjoy running! (PB: Half marathon 1:55:35)</li>-->
                    <!--</ul>-->
                <!--</div>-->

            </div>
        </div>


        <!-- Optional JavaScript -->
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
        <script src="assets/js/anchor.min.js"></script>

        <script>
            anchors.add("h2, h3, h4, h5");
            var liAnchors = new AnchorJS({
                placement: 'right',
                icon: '¶'
            });
            liAnchors.add('.anchor-strong');

            var n_arxiv = $(".li-arxiv").length + 1;
            var n_conf = $(".li-conference").length + $(".li-conference-workshop").length + 1;
            var n_ws = $(".li-workshop").length + $(".li-conference-workshop").length + 1;
            var n_journal = $(".li-journal").length + 1;
            var n_wsorig = $(".li-workshop-orig").length + 1;
            var n_tt = $(".li-tutorial").length + 1;
            $("head").append("<style>body {counter-reset: arxivcounter " + n_arxiv + " confcounter " + n_conf + " wscounter " + n_ws + " journalcounter " + n_journal + " wsocounter " + n_wsorig + " ttcounter " + n_tt + "; padding-left: 0;}</style>")
        </script>
    </body>
</html>
